# 5th place solution overview

[5th place solution](https://www.kaggle.com/competitions/stable-diffusion-image-to-prompts/discussion/410688)

## Solution summary

Our approach involved training multiple image models to predict prompt embeddings (generated by a sentence transformer) directly from images, then ensembling the predictions from these models. The final submission was an ensemble of four models:

- eva02_large
- convnext_xxlarge
- convnext_large
- vit_large

Initially, in the early stages of the competition, we experimented with models that predicted the image prompts themselves. However, we found that the method of directly predicting prompt embeddings yielded better scores. Consequently, from the midpoint of the competition onwards, we focused on improving the method of direct embedding prediction.


## Dataset Generation

We generated approximately 5 million pairs of prompt-image sets (some images were generated multiple times from the same prompt) as detailed in the table below:

|dataset name     |# of prompts|images per prompt|Total images|
|:----------------|-----------:|----------------:|-----------:|
|cc3m             |     249,593|                3|     748,781|
|lexica           |      67,101|                3|     201,303|
|diffusiondb      |     326,035|                3|     978,105|
|cc3m part2       |   1,166,852|                1|   1,166,852|
|diffusiondb part2|   1,371,480|                1|   1,371,480|
|mscoco           |     197,527|                3|     592,583|
|Total:           |   3,378,588|             1or3|   5,059,104|

During training, for samples with three images corresponding to a prompt, we randomly selected one image to ensure that duplicate images from the same prompt were not used within the same epoch.

Dataset references
- cc3m: https://ai.google.com/research/ConceptualCaptions/download
- lexica: https://www.kaggle.com/datasets/safavieh/gustavosta-stable-diffusion-prompts
- diffusiondb: https://huggingface.co/datasets/poloclub/diffusiondb
- mscoco: https://cocodataset.org/


## Validation Strategy

To partition the generated image-prompt pairs for training and evaluation, we first calculated the prompt embeddings using a sentence transformer. We then grouped samples with similar prompt embeddings. After grouping samples with a cosine similarity of 0.7 or higher in their embeddings, we divided the dataset into 10 folds based on these groups using GroupKFold. Model training was conducted on all folds except fold0, which was reserved for evaluation.

Furthermore, during the creation of the cc3m and diffusiondb datasets, we precomputed the cosine similarity of prompt embeddings. For prompts with a similarity of 0.9 or above, we only used one for image generation. From the midpoint of the competition, we also began to utilize the remaining prompts that we had not used before, creating the cc3m_part2 and diffusiondb_part2 datasets.

## Model Architecture

Our model architecture is straightforward, consisting of a backbone connected to a 384-dimensional fully connected (FC) layer (without bias), which matches the output dimensions of the sentence transformer.

Some models have a larger input image resolution as shown below.

|model           |pretrained backbone                             |input size          |
|:---------------|:-----------------------------------------------|:-------------------|
|eva02_large     |eva02_large_patch14_448.mim_m38m_ft_in22k (timm)|448 (pretrained@448)|
|convnext_xxlarge|laion2b_s34b_b82k_augreg_rewind (open_clip)     |384 (pretrained@256)|
|convnext_large  |laion2b_s29b_b131k_ft_soup (open_clip)          |384 (pretrained@320)|
|vit_large       |laion2b_s32b_b82k (open_clip)                   |336 (pretrained@224)|

The ViT model of open_clip can change the input image size as follows.

```
backbone = open_clip_model.visual
hgrid, wgrid = backbone.grid_size
hpatch = input_height // hgrid
wpatch = input_width // wgrid
backbone.patch_size = (hpatch, wpatch)
backbone.conv1.kernel_size = (hpatch, wpatch)
backbone.conv1.stride = (hpatch, wpatch)
```

## Training Process

We used the PyTorch framework for training our models, with the following configurations:

- 10 or 15 epochs with Distributed Data Parallel (DDP), Automatic Mixed Precision (AMP), and Gradient Checkpointing
- CosineEmbeddingLoss as the loss function
- MADGRAD as the optimizer
- Warmup 1 or 3 epochs training only FC layer with the backbone frozen (learning rate: from 1e-2 to 1e-4)
- Cosine Learning Rate Scheduler with learning rates ranging from 1e-4 to 1e-7.
- Finetune 3 or 5 epochs with all dataset.

As for data augmentation, we implemented a relatively light configuration as follows:

- No Rotation
- No Horizontal Flip
- RandomResizedCrop with a scale of 0.5 to 1.0
- ColorJitter (brightness=0.05, contrast=0.05, saturation=0.05, hue=0.05)


Until midway through the competition, we trained our models using only three datasets: cc3m, lexica, and diffusiondb.

|model           |epochs|warmup|initial lr|final lr|dataset                |
|:---------------|-----:|-----:|---------:|-------:|:----------------------|
|eva02_large     |10    |1     |1e-4      |1e-6    |cc3m,lexica,diffusiondb|
|convnext_xxlarge|10    |1     |1e-4      |1e-6    |cc3m,lexica,diffusiondb|
|convnext_large  |15    |3     |1e-4      |1e-6    |cc3m,lexica,diffusiondb|
|vit_large       |15    |3     |1e-4      |1e-6    |cc3m,lexica,diffusiondb|


From the midpoint onwards, we included the cc3m_part2, diffusiondb_part2, and mscoco datasets for fine-tuning the model.

|model           |epochs|warmup|initial lr|final lr|dataset|
|:---------------|-----:|-----:|---------:|-------:|:------|
|eva02_large     |3     |0     |1e-6      |1e-7    |all    |
|convnext_xxlarge|3     |0     |1e-6      |1e-7    |all    |
|convnext_large  |5     |0     |1e-5      |1e-7    |all    |
|vit_large       |5     |0     |1e-5      |1e-7    |all    |


## Ensemble Strategy

In our final submission, we utilized four models. Also, for each model, we used four or five different cross-validation folds, and thus we created an ensemble of 18 folds across the four models.

|model           |folds    |
|:---------------|:--------|
|eva02_large     |0,2,2*,5,9|
|convnext_xxlarge|0,1,6,6*,7|
|convnext_large  |0,1,2,3  |
|vit_large       |0,3,4,8  |

Folds marked with * are the ones where we fine-tuned for only 1 epoch without data augmentation.

We had models validated on fold0 prepared for each model, and the weights of the ensemble were adjusted using these fold0 validation data.


We also experimented with determining weights not just on a per-model basis, but along each of the 384 output dimensions for each model (amounting to 384x4 weights), which were also determined through training. However, the training didn't result in significant variability across the weights for specific dimensions, and the outcome was almost the same as when determining a single weight per model.

Upon submitting both methods and comparing their scores, the method using 384 weights per model resulted in a slight score increase of +0.0002. Therefore, we implemented the latter method for our final submission. However, it's worth noting that this improvement is likely within the margin of error.



## Things that didn't work well for us

- Mixup
- GeM (Generalized Mean) Pooling
- GGeM pooling ([Group Generalized Mean Pooling for Vision Transformer](https://arxiv.org/abs/2212.04114))
- Triplet Attention ([Convolutional Triplet Attention Module](https://arxiv.org/abs/2010.03045))
- Image captioning like BLIP, BLIP2, CoCa
  - Predicting the prompt embedding directly was more effective in terms of both computational cost and score.
- Weighting loss based on scores from the model's OOF predictions (loss considering easy or difficult samples)
- kNN approach using a pretrained clip model
  - Although improvements were observed when ensembled with the prediction results of a single model, no effect was observed when added to the results of the 4-model ensemble used in the final submission.
- Cosine similarity loss for all sample combinations (details to follow)

### Cosine Similarity Loss for All Sample Combinations

When using CosineEmbeddingLoss, the loss calculation is performed for each image-prompt pair, without considering the relationships between different pairs. To address this, we tried to implement a loss that would bring the cosine similarity of all pairs of model outputs closer to the cosine similarity of all pairs of corresponding targets within a mini-batch, considering all combinations of samples.

However, we didn't adopt this approach as it didn't improve performance, whether used alone or in combination with CosineEmbeddingLoss.

